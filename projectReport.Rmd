---
title: "Practical Machine Learning Course Project Report"
author: "[Jim Leach](https://twitter.com/leach_jim)"
date: "`r I(Sys.Date())`"
output: html_document
---

****

## Introduction

This report has been created as part of the [Practical Machine Learning Coursera Course](https://www.coursera.org/course/predmachlearn) offered by John's Hopkins University as part of the [Data Science Specialisation](https://www.coursera.org/specialization/jhudatascience/1?utm_medium=courseDescripTop).

The data for this project came from from a study that used accelerometer data to track how a group of individuals performed a number of weight lifting exercises in one of five distinct ways. Further information about this study is available [here](http://groupware.les.inf.puc-rio.br/har).

The objective of the assignment was to use this data to build a model to predict the way in which an individual had performed the exercise.  

****

## Executive Summary

* The Random Forest algorithm was chosen as the predictive method. This was due to it's robustness to a range of classification problems.

* When training the algorithm, the estimated out-of-bag/out-of-sample error rate was found to be approximately 0.5%.

* When testing the performance of the model on unseen data, the accuracy was found to be 99.7% [99.5%, 99.9%].

****

## Approach

### Step 0 - Set up environment

Firstly, the R working environment was established and the necessary packages loaded.

```{r setup, message=FALSE,warning=FALSE}
# clear out environment
  rm(list = ls())

# load packages
  library(magrittr, quietly = T)
  library(caret, quietly = T)
  library(randomForest, quietly = T)
```

### Step 1 - Getting and loading the data

The training and testing/validation data sets were downloaded and saved locally.

```{r getData, message=FALSE, warning=FALSE, cache=TRUE}
# set destinations
  trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
  testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

# make a folder to store data
  if(!file.exists("./data")){
    dir.create("./data")
  }
# train
  if(!file.exists("./data/train.csv")){
    download.file(trainURL, "./data/train.csv", method = "curl")
  }
# test
  if(!file.exists("./data/test.csv")){
    download.file(testURL, "./data/test.csv", method = "curl")
  }

  rm(testURL)
  rm(trainURL)
```

The training data were read in to R ready for the analysis. Note that following a manual inspection of the data prior to loading, it was discovered that missing data could be represented by either `NA` or `#DIV/0!`.

```{r readData, message=FALSE,warning=FALSE, cache=TRUE}
# read training data (save test for when we actually need it!)
  data <- read.csv("./data/train.csv", header = T, na.strings = c("#DIV/0!", "NA"))
```

### Step 2 - Exploring and cleaning the data
```{r explore, message=FALSE,warning=FALSE, eval=FALSE}
# quick overview
  str(data) 
  summary(data)
```

Looking at simple summaries generated by R using the `str()` and `summary()` commands showed that:

* The first seven data fields were descriptive/record idendifiers; and
* There were several fields with a very high proportion of missing values. 

As such, it was decided that the descriptive/identifier fields would be removed from the analysis and, further, that only fields with fewer than 30% missing values would be included for the analysis.

```{r clean,message=FALSE, warning=FALSE}
# remove the junk fields
  data <- data[,-c(1:7)]
  data <- data[,colSums(is.na(data)) < 0.3*nrow(data)]
```

The resulted in a clean data set with `r I(ncol(data)-1)` predictors to be used in the model.

### Step 3 - Build the model

Firstly, the data were split in to a training and test set using the functionality provided by the `caret` package.
```{r dataPartition, message=FALSE,warning=FALSE}
set.seed(19891110)
  index <- createDataPartition(data$classe, p = 0.8, list = F)
  train <- data[index,]
  test <- data[-index,]
plot <- featurePlot(x = train[,-53], y = train$classe, plot = "box")
```

Following this, the `randomForest` function was used to fit the algorithm using all variables as predictors. All variables were chosen as predictors as, following examination of an exploratory features plot (Appendix A), no significant patterns were seen that indicated certain features would be better predictors that others.

It should be noted that this report presents the final parameters used in the model (i.e. `ntree` set to 201), the actual fitting of the model was a more iterative process and is not presented here for brevity's sake.

```{r fitForest, message=FALSE, warning=FALSE, cache=FALSE}
set.seed(19891110)
# fit the model - try random forest first as the data are not large and it's a robust method
rf <- randomForest(classe ~ ., data = train, ntree = 201)
```

The output of this model is as follows:
```{r rfOut, echo=FALSE}
rf
```

As can be seen, this is an effective model, with an estimated out-of-bag error rate of 0.52%.

***

## Results - Cross-validate the model

Finally, the held-out testing data set was used to evaluate the performance of the model on unseen data.

```{r evaluate, message=FALSE,warning=FALSE}
library(knitr)
testOutcome <- predict(rf, test)
confMatrix <- confusionMatrix(testOutcome, test$classe)
stats <- kable(confMatrix$overall %>% as.data.frame)
matrix <- kable(confMatrix$table)
```

The output of the confusion matrix is as follows (note that the columns represent the true values, the rows the model-predicted outcomes):
```{r confMatrix, message=FALSE,warning=FALSE, echo=FALSE}
matrix
```

Moreover, the performance of the model was high:
```{r stats, message=FALSE, warning=FALSE,echo=FALSE}
stats
```

With a statistically significant accuracy value of `r paste0(round(100*I(confMatrix$overall[1]),1),"%")` [`r paste0(round(100*I(confMatrix$overall[3]),1),"%")`, `r paste0(round(100*I(confMatrix$overall[4]),1),"%")`], the model was seen to be a highly effective predictor.

***

## Appendices

### Appendix A - Feature Plot of Training Data

```{r featurePlot, echo=FALSE, warning=FALSE,message=FALSE}
plot
```

***

## Notes

The author of this report can be contacted via [twitter](https://twitter.com/leach_jim).




